{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12609125,"sourceType":"competition"},{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":12204996,"sourceType":"datasetVersion","datasetId":7688334},{"sourceId":12205277,"sourceType":"datasetVersion","datasetId":7688411},{"sourceId":12207625,"sourceType":"datasetVersion","datasetId":7690162},{"sourceId":12282442,"sourceType":"datasetVersion","datasetId":7740553},{"sourceId":445447,"sourceType":"modelInstanceVersion","modelInstanceId":361697,"modelId":382674},{"sourceId":446667,"sourceType":"modelInstanceVersion","modelInstanceId":362740,"modelId":383637}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n!pip install /kaggle/input/optuna/optuna_integration-4.2.1-py3-none-any.whl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-25T16:20:44.045271Z","iopub.execute_input":"2025-06-25T16:20:44.046031Z","iopub.status.idle":"2025-06-25T16:20:54.321451Z","shell.execute_reply.started":"2025-06-25T16:20:44.045983Z","shell.execute_reply":"2025-06-25T16:20:54.320781Z"}},"outputs":[{"name":"stdout","text":"Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\nInstalling collected packages: rdkit\nSuccessfully installed rdkit-2025.3.3\nProcessing /kaggle/input/optuna/optuna_integration-4.2.1-py3-none-any.whl\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (from optuna-integration==4.2.1) (4.3.0)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (6.9.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (25.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (2.0.40)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (4.67.1)\nRequirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna->optuna-integration==4.2.1) (6.0.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna-integration==4.2.1) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna->optuna-integration==4.2.1) (4.13.2)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->optuna-integration==4.2.1) (3.1.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->optuna->optuna-integration==4.2.1) (2.4.1)\nRequirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from Mako->alembic>=1.5.0->optuna->optuna-integration==4.2.1) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna->optuna-integration==4.2.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->optuna->optuna-integration==4.2.1) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->optuna->optuna-integration==4.2.1) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->optuna->optuna-integration==4.2.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->optuna->optuna-integration==4.2.1) (2024.2.0)\nInstalling collected packages: optuna-integration\nSuccessfully installed optuna-integration-4.2.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ====================================================\n# 셀 2: 라이브러리 및 기본 설정\n# ====================================================\nprint(\"라이브러리 임포트 및 기본 설정 중...\")\n\n# --- 데이터 처리 및 수학 연산 ---\nimport pandas as pd\nimport numpy as np\nimport joblib # 모델 저장 및 로딩\n\n# --- 시스템 및 유틸리티 ---\nfrom tqdm.auto import tqdm\nimport random\nimport os\nimport gc # Garbage Collection: 메모리 관리\nimport warnings\n\n# --- RDKit: 분자 화학 정보학 ---\nfrom rdkit import Chem, rdBase\nfrom rdkit.Chem import Descriptors, AllChem\nfrom rdkit.Chem.rdMolDescriptors import GetMorganFingerprintAsBitVect\n\n# --- 머신러닝 모델 ---\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.feature_selection import SelectKBest, mutual_info_regression\nfrom catboost import CatBoostRegressor\n\n# --- 하이퍼파라미터 최적화 ---\nimport optuna\n\n# --- 경고 메시지 비활성화 ---\nrdBase.DisableLog('rdApp.warning') \nwarnings.filterwarnings('ignore')\n\nprint(\"임포트 및 기본 설정 완료.\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 3: 환경 설정 (Configuration) 및 시드 고정\n# ====================================================\nclass CFG:\n    # 경로 설정\n    TRAIN_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/train.csv'\n    TEST_PATH = '/kaggle/input/neurips-open-polymer-prediction-2025/test.csv'\n    \n    # 캐시 파일 경로\n    CACHE_DIR = '/kaggle/working/cache/'\n    TRAIN_FEAT_PATH = os.path.join(CACHE_DIR, 'train_features_final.pkl')\n    TEST_FEAT_PATH = os.path.join(CACHE_DIR, 'test_features_final.pkl')\n\n    # 디바이스 설정\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    # 모델 및 훈련 파라미터\n    N_FOLDS = 10\n    TARGETS = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n    RANDOM_STATE = 42\n    N_FEATURES = 1000\n    \n    # 신경망 모델 파라미터\n    NN_BATCH_SIZE = 64\n    NN_EPOCHS = 100\n    NN_LR = 1e-4\n\nprint(f\"디바이스 설정: {CFG.DEVICE}\")\n\n# 재현성을 위한 시드 고정\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n\nseed_everything(CFG.RANDOM_STATE)\nprint(f\"시드 값 {CFG.RANDOM_STATE}로 고정 완료.\")\nos.makedirs(CFG.CACHE_DIR, exist_ok=True)\nprint(f\"캐시 디렉토리 '{CFG.CACHE_DIR}' 생성/확인 완료.\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 4: 특성 공학 클래스 정의\n# ====================================================\nclass AdvancedFeaturizer:\n    def __init__(self, n_features=1000):\n        self.n_features = n_features\n        self.feature_selector = None\n        self.base_feature_names = []\n\n    def _get_features(self, smiles):\n        try:\n            mol = Chem.MolFromSmiles(smiles)\n            if mol is None: return np.zeros(1224)\n            \n            descriptors = [func(mol) for _, func in Descriptors.descList]\n            morgan_fp = list(GetMorganFingerprintAsBitVect(mol, 2, nBits=1024))\n            return np.array(descriptors + morgan_fp, dtype=np.float32)\n        except:\n            return np.zeros(1224)\n\n    def fit_transform(self, df, targets=None):\n        # 기본 특성 이름 설정\n        if not self.base_feature_names:\n            descriptor_names = [d[0] for d in Descriptors.descList]\n            morgan_names = [f'morgan_{i}' for i in range(1024)]\n            self.base_feature_names = descriptor_names + morgan_names\n        \n        features = np.array([self._get_features(s) for s in tqdm(df['SMILES'], desc=\"특성 추출 중\")])\n        features_df = pd.DataFrame(features, columns=self.base_feature_names)\n        \n        # 데이터 정제\n        features_df = features_df.apply(pd.to_numeric, errors='coerce')\n        features_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        features_df.fillna(0, inplace=True)\n        \n        # 특성 선택 (훈련 데이터에만 적용)\n        if targets is not None and self.feature_selector is None:\n            print(f\"{self.n_features}개 특성 선택...\")\n            ffv_mask = ~np.isnan(targets[:, 1])\n            if ffv_mask.sum() > 100:\n                self.feature_selector = SelectKBest(mutual_info_regression, k=min(self.n_features, features_df.shape[1]))\n                self.feature_selector.fit(features_df.loc[ffv_mask], targets[ffv_mask, 1])\n        \n        if self.feature_selector:\n            features_df = pd.DataFrame(self.feature_selector.transform(features_df), \n                                     columns=self.feature_selector.get_feature_names_out())\n        \n        return features_df\n\nprint(\"특성 공학 클래스 'AdvancedFeaturizer' 정의 완료.\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 5: PyTorch 모델 및 데이터셋 정의\n# ====================================================\nclass PolymerDataset(Dataset):\n    def __init__(self, features, targets):\n        self.features = torch.tensor(features, dtype=torch.float32)\n        self.targets = torch.tensor(targets, dtype=torch.float32)\n    def __len__(self): return len(self.features)\n    def __getitem__(self, idx): return self.features[idx], self.targets[idx]\n\nclass PolymerTransformer(nn.Module):\n    def __init__(self, input_dim, embed_dim=256, num_heads=8, num_layers=4, dropout=0.1):\n        super().__init__()\n        self.input_proj = nn.Linear(input_dim, embed_dim)\n        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout, batch_first=True)\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.output_heads = nn.ModuleList([nn.Linear(embed_dim, 1) for _ in range(len(CFG.TARGETS))])\n\n    def forward(self, x):\n        x = self.input_proj(x.unsqueeze(1))\n        x = self.transformer(x).squeeze(1)\n        return {CFG.TARGETS[i]: head(x).squeeze(-1) for i, head in enumerate(self.output_heads)}\n\nprint(\"PyTorch 관련 클래스 및 함수 정의 완료.\")\nprint(\"-\" * 50)\n\n# ====================================================\n# 셀 6: 데이터 로딩 및 전처리 실행\n# ====================================================\ndef get_processed_data():\n    if os.path.exists(CFG.TRAIN_FEAT_PATH) and os.path.exists(CFG.TEST_FEAT_PATH):\n        print(\"캐시된 특성 파일 로딩...\")\n        train_df = pd.read_csv(CFG.TRAIN_PATH)\n        test_df = pd.read_csv(CFG.TEST_PATH)\n        train_features = joblib.load(CFG.TRAIN_FEAT_PATH)\n        test_features = joblib.load(CFG.TEST_FEAT_PATH)\n        return train_df, test_df, train_features, test_features\n    \n    print(\"데이터 로드 및 전처리 시작...\")\n    train_df = pd.read_csv(CFG.TRAIN_PATH)\n    test_df = pd.read_csv(CFG.TEST_PATH)\n    \n    all_smiles = pd.concat([train_df[['SMILES']], test_df[['SMILES']]], ignore_index=True)\n    featurizer = AdvancedFeaturizer(n_features=CFG.N_FEATURES)\n    train_targets_for_selection = train_df[CFG.TARGETS].values.astype(np.float32)\n    \n    all_features = featurizer.fit_transform(all_smiles, np.vstack([train_targets_for_selection, np.full((len(test_df), 5), np.nan)]))\n\n    train_features = all_features.iloc[:len(train_df)]\n    test_features = all_features.iloc[len(train_df):]\n    \n    joblib.dump(train_features, CFG.TRAIN_FEAT_PATH)\n    joblib.dump(test_features, CFG.TEST_FEAT_PATH)\n    \n    return train_df, test_df, train_features, test_features\n\ntrain_df, test_df, train_features, test_features = get_processed_data()\nprint(f\"훈련 특성 데이터 형태: {train_features.shape}\")\nprint(f\"테스트 특성 데이터 형태: {test_features.shape}\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 7: 1단계 - 유사 레이블 생성을 위한 CatBoost 모델 훈련\n# ====================================================\nprint(\"===== 1단계: 유사 레이블 생성 모델 훈련 시작 =====\")\npseudo_train_df = train_df.copy()\ncatboost_oof_maes = {}\n\nfor target in CFG.TARGETS:\n    print(f\"\\n--- 유사 레이블 생성을 위한 '{target}' 모델 훈련 ---\")\n    \n    not_na_idx = train_df[target].notna()\n    X = train_features[not_na_idx].values\n    y = train_df.loc[not_na_idx, target].values\n    X_to_predict = train_features[~not_na_idx].values\n    \n    kf = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\n    oof_preds = np.zeros(len(X))\n    pseudo_labels = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y[train_idx], y[val_idx]\n\n        scaler = StandardScaler()\n        X_train_s = scaler.fit_transform(X_train)\n        X_val_s = scaler.transform(X_val)\n        \n        model = CatBoostRegressor(iterations=4000, learning_rate=0.02, depth=8, loss_function='MAE', verbose=0, task_type='GPU' if CFG.DEVICE.type=='cuda' else 'CPU')\n        model.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], early_stopping_rounds=100, verbose=0)\n        \n        oof_preds[val_idx] = model.predict(X_val_s)\n        \n        if X_to_predict.shape[0] > 0:\n            X_to_predict_s = scaler.transform(X_to_predict)\n            pseudo_labels.append(model.predict(X_to_predict_s))\n\n    catboost_oof_maes[target] = mean_absolute_error(y, oof_preds)\n    print(f\"'{target}' 모델 OOF MAE (유사 레이블 생성용): {catboost_oof_maes[target]:.5f}\")\n    \n    if len(pseudo_labels) > 0:\n        pseudo_train_df.loc[~not_na_idx, target] = np.mean(pseudo_labels, axis=0)\n\nprint(\"\\n유사 레이블 생성 완료. 훈련 데이터가 보강되었습니다.\")\nprint(f\"보강 후 훈련 데이터 NaN 개수:\\n{pseudo_train_df[CFG.TARGETS].isnull().sum()}\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 8: 2단계 - 최종 앙상블 모델 훈련 (CatBoost 및 Transformer)\n# ====================================================\nprint(\"===== 2단계: 최종 앙상블 모델 훈련 시작 =====\")\n\n# 최종 예측 결과를 저장할 데이터프레임\noof_catboost_final = pd.DataFrame(index=train_df.index)\noof_transformer_final = pd.DataFrame(index=train_df.index)\nsubmission_catboost = pd.DataFrame({'id': test_df['id']})\nsubmission_transformer = pd.DataFrame({'id': test_df['id']})\n\n# 최종 훈련에 사용할 보강된 데이터\nfinal_train_targets = pseudo_train_df[CFG.TARGETS].values\n\n# --- 2.1 CatBoost 모델 훈련 (보강된 데이터 사용) ---\nfor target in CFG.TARGETS:\n    print(f\"\\n--- 최종 CatBoost '{target}' 모델 훈련 ---\")\n    kf = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\n    y_target = final_train_targets[:, CFG.TARGETS.index(target)]\n    oof_preds = np.zeros(len(train_features))\n    test_preds_folds = []\n    \n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_features, y_target)):\n        X_train, X_val = train_features.values[train_idx], train_features.values[val_idx]\n        y_train, y_val = y_target[train_idx], y_target[val_idx]\n\n        scaler = StandardScaler()\n        X_train_s = scaler.fit_transform(X_train)\n        X_val_s = scaler.transform(X_val)\n        \n        model = CatBoostRegressor(iterations=4000, learning_rate=0.02, depth=8, loss_function='MAE', verbose=0, task_type='GPU' if CFG.DEVICE.type=='cuda' else 'CPU')\n        model.fit(X_train_s, y_train, eval_set=[(X_val_s, y_val)], early_stopping_rounds=100, verbose=0)\n        \n        oof_preds[val_idx] = model.predict(X_val_s)\n        test_preds_folds.append(model.predict(scaler.transform(test_features.values)))\n\n    oof_catboost_final[target] = oof_preds\n    submission_catboost[target] = np.mean(test_preds_folds, axis=0)\n    print(f\"최종 CatBoost '{target}' 모델 OOF MAE: {mean_absolute_error(y_target, oof_preds):.5f}\")\n\n# --- 2.2 Transformer 모델 훈련 (보강된 데이터 사용) ---\nprint(\"\\n--- 최종 Transformer 모델 훈련 ---\")\nkf = KFold(n_splits=CFG.N_FOLDS, shuffle=True, random_state=CFG.RANDOM_STATE)\noof_preds_nn = np.zeros_like(final_train_targets)\ntest_preds_folds_nn = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_features, final_train_targets)):\n    print(f\"  Fold {fold+1}/{CFG.N_FOLDS} 훈련 중...\")\n    X_train, X_val = train_features.values[train_idx], train_features.values[val_idx]\n    y_train, y_val = final_train_targets[train_idx], final_train_targets[val_idx]\n\n    scaler = StandardScaler()\n    X_train_s = scaler.fit_transform(X_train)\n    X_val_s = scaler.transform(X_val)\n    \n    train_dataset = PolymerDataset(X_train_s, y_train)\n    val_dataset = PolymerDataset(X_val_s, y_val)\n    train_loader = DataLoader(train_dataset, batch_size=CFG.NN_BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=CFG.NN_BATCH_SIZE, shuffle=False)\n    \n    model = PolymerTransformer(input_dim=CFG.N_FEATURES).to(CFG.DEVICE)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.NN_LR)\n    criterion = nn.L1Loss() # MAE Loss\n\n    best_val_loss = float('inf')\n    for epoch in range(CFG.NN_EPOCHS):\n        model.train()\n        for features, targets in train_loader:\n            features, targets = features.to(CFG.DEVICE), targets.to(CFG.DEVICE)\n            optimizer.zero_grad()\n            outputs = model(features)\n            loss = sum(criterion(outputs[t], targets[:, i]) for i, t in enumerate(CFG.TARGETS))\n            loss.backward()\n            optimizer.step()\n    \n    model.eval()\n    val_preds_fold = np.zeros_like(y_val)\n    with torch.no_grad():\n        outputs = model(torch.tensor(X_val_s, dtype=torch.float32).to(CFG.DEVICE))\n        for i, t in enumerate(CFG.TARGETS):\n            val_preds_fold[:, i] = outputs[t].cpu().numpy()\n    oof_preds_nn[val_idx] = val_preds_fold\n    \n    test_preds_fold = np.zeros((len(test_features), len(CFG.TARGETS)))\n    with torch.no_grad():\n        outputs = model(torch.tensor(scaler.transform(test_features.values), dtype=torch.float32).to(CFG.DEVICE))\n        for i, t in enumerate(CFG.TARGETS):\n            test_preds_fold[:, i] = outputs[t].cpu().numpy()\n    test_preds_folds_nn.append(test_preds_fold)\n    \noof_transformer_final[CFG.TARGETS] = oof_preds_nn\nsubmission_transformer[CFG.TARGETS] = np.mean(test_preds_folds_nn, axis=0)\n\nprint(\"\\n최종 모델 훈련 완료.\")\nprint(\"-\" * 50)\n\n\n# ====================================================\n# 셀 9: 3단계 - 적응형 앙상블 및 최종 제출\n# ====================================================\nprint(\"===== 3단계: 적응형 앙상블 및 최종 제출 파일 생성 =====\")\n\n# 각 모델의 OOF 점수 계산\ncatboost_final_scores = {t: mean_absolute_error(final_train_targets[:, i], oof_catboost_final[t]) for i, t in enumerate(CFG.TARGETS)}\ntransformer_final_scores = {t: mean_absolute_error(final_train_targets[:, i], oof_transformer_final[t]) for i, t in enumerate(CFG.TARGETS)}\n\nprint(\"\\n--- 최종 OOF MAE 점수 ---\")\nprint(\"CatBoost:\", catboost_final_scores)\nprint(\"Transformer:\", transformer_final_scores)\n\n# 가중 평균을 위한 가중치 계산\nfinal_predictions = pd.DataFrame({'id': test_df['id']})\nprint(\"\\n--- 앙상블 가중치 ---\")\nfor target in CFG.TARGETS:\n    score1 = catboost_final_scores[target]\n    score2 = transformer_final_scores[target]\n    \n    # 더 좋은 성능(낮은 MAE)에 높은 가중치 부여\n    w1 = 1 / (score1 + 1e-6)\n    w2 = 1 / (score2 + 1e-6)\n    w_sum = w1 + w2\n    \n    weight_catboost = w1 / w_sum\n    weight_transformer = w2 / w_sum\n    \n    print(f\"'{target}': CatBoost 가중치={weight_catboost:.3f}, Transformer 가중치={weight_transformer:.3f}\")\n    \n    final_predictions[target] = (submission_catboost[target] * weight_catboost + \n                                 submission_transformer[target] * weight_transformer)\n\n# 최종 제출 파일 저장\nfinal_predictions.to_csv('submission.csv', index=False)\nprint(\"\\n최종 제출 파일 'submission.csv'이 성공적으로 생성되었습니다.\")\nprint(\"\\n최종 제출 파일 샘플:\")\nprint(final_predictions.head())\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T16:30:44.202977Z","iopub.execute_input":"2025-06-25T16:30:44.203668Z"}},"outputs":[{"name":"stdout","text":"라이브러리 설치 중...\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0m설치 완료.\n--------------------------------------------------\n라이브러리 임포트 및 기본 설정 중...\n임포트 및 기본 설정 완료.\n--------------------------------------------------\n디바이스 설정: cuda\n시드 값 42로 고정 완료.\n캐시 디렉토리 '/kaggle/working/cache/' 생성/확인 완료.\n--------------------------------------------------\n특성 공학 클래스 'AdvancedFeaturizer' 정의 완료.\n--------------------------------------------------\nPyTorch 관련 클래스 및 함수 정의 완료.\n--------------------------------------------------\n데이터 로드 및 전처리 시작...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"특성 추출 중:   0%|          | 0/7976 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5a6974edddb41bd899e20a4788daafc"}},"metadata":{}},{"name":"stdout","text":"1000개 특성 선택...\n훈련 특성 데이터 형태: (7973, 1000)\n테스트 특성 데이터 형태: (3, 1000)\n--------------------------------------------------\n===== 1단계: 유사 레이블 생성 모델 훈련 시작 =====\n\n--- 유사 레이블 생성을 위한 'Tg' 모델 훈련 ---\n","output_type":"stream"},{"name":"stderr","text":"Default metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\nDefault metric period is 5 because MAE is/are not implemented for GPU\n","output_type":"stream"}],"execution_count":null}]}